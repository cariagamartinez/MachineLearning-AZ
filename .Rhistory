# Así training_set será un subset de dataset y comparará los datos generados por sample.split
# en aquellos que split sea TRUE, serán guardados en training_set y lo mismo con testing_set
training_set = subset (dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores. Nuevamente,la idea es que las variables no dominen sobre las otras.
# Podemos estandarizar o normalizar. Vamos a usar una función del paquete base.
# Pero en este caso tenemos que recordar que solo podemos escalar las columnas numéricas y la columna 1 y 4 son niveles de factor.
# Por eso, ahora vamos a escalar solamente las columnas 2 y 3. Todas las filas se indica con la coma directamente
# (a diferencia que Python que requiere :) y luego indicamos las dos columnas a escalar
training_set[, 2:3] = scale(training_set[, 2:3])
testing_set[, 2:3] = scale(testing_set[, 2:3])
library(caTools)
#Plantilla para el preprocesado de datos
#Importar el dataset
#Creamos también una variable para "guardar" el conjunto de datos.
dataset = read.csv("Data.csv")
#Tratamiento de los valores NA
# Con el signo del dólar podemos acceder a los objetos dentro de la variable dataset (USAR EL TABULADOR).
# Luego nos preguntamos si hay algún NA en la fila de Age y vamos a usar la función ifelse. Esta función ejecuta la primera parte
# del código si la condición es verdadera o bien, la segunda si la condición es falsa.
# La pregunta que haremos a la función ifelse es saber si hay NA en dataset$Age --> is.na (Hay NAs?).
# Si es verdadera se ejecutará lo que haya entre la primera y la segunda coma,
# y si es falso, se ejecuta lo que hay luego de la segunda coma hasta el paréntesis final. Quedaría:
# dataset$Age = ifelse(is.na(dataset$Age), Primera_condicion , Segunda_condicion )
# El primer parámetro será la función ave, que nos permitirá obtener un promedio del factor (columna)
# mediante la función FUN que a cada x, lo reemplaza con la media de la columna, sin incluir los NAs,
# es decir --> na.rm = TRUE. Quedaría:
#dataset$Age = ifelse(is.na(dataset$Age),
#               ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
#               , Segunda_condicion)
# Si no se encuentra ningún NA, entonces se no se tocaría el código y se mantendría el dataset$Age. Quedaría:
#dataset$Age = ifelse(is.na(dataset$Age),
#               ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
#               , dataset$Age)
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
# Hacemos lo mismo para dataset$salary
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
# Codificar las variables categóricas. En este caso vamos a usar la función factor.
# Esta función representa categorías directamente, por lo que necesita que se indique los levels
# y luego las etiquetas numéricas que se asignen a cada level.
# factor (variable , levels = c(), labels = c())
# En este caso, el programa sabe que la función "factor" representa categorías
# por lo que no es necesario indicar que las etiquetas numéricas no son realmente "números" (que se puedan sumar, etc.)
# Notar que tanto levels como labels son vectores (c)
dataset$Country = factor(dataset$Country,
levels = c("France" , "Spain" , "Germany"),
labels = c(1 , 2 , 3))
# Ahora también hacemos lo mismo con el factor "Purchase" y transformamos a factor 0 el "no" y 1 el "yes"
dataset$Purchased = factor(dataset$Purchased,
levels = c("No" , "Yes"),
labels = c(0 , 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test.
# Vamos a usar caTools
#install.packages("caTools") #Lo he tenido que instalar porque no lo tenía.
# Para cargar la librería usamos library(caTools).
# Como no tenemos la opción de indicar una semilla en la función sample.split, vamos a generarla antes con set.seet(entero)
# En este caso, siempre que usemos la misma semilla, se generará la misma división de datos.
# También vemos que esta función solo acepta la variable a predecir (y = purchased)
# y que el SpliRatio indica la cantidad de datos para entrenamiento (en Python indicamos la cantidad para test).
# Al final, guardamos el return en una variable llamada split
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# Como todo sigue guardado en dataset y no se generan nuevas variables, ahora vamos a generar un subconjunto del dataset original
# donde recogeremos los valores en los que la variable split sea TRUE o FALSE y los guardaremos en una nueva variable.
# Así training_set será un subset de dataset y comparará los datos generados por sample.split
# en aquellos que split sea TRUE, serán guardados en training_set y lo mismo con testing_set
training_set = subset (dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores. Nuevamente,la idea es que las variables no dominen sobre las otras.
# Podemos estandarizar o normalizar. Vamos a usar una función del paquete base.
# Pero en este caso tenemos que recordar que solo podemos escalar las columnas numéricas y la columna 1 y 4 son niveles de factor.
# Por eso, ahora vamos a escalar solamente las columnas 2 y 3. Todas las filas se indica con la coma directamente
# (a diferencia que Python que requiere :) y luego indicamos las dos columnas a escalar
training_set[, 2:3] = scale(training_set[, 2:3])
testing_set[, 2:3] = scale(testing_set[, 2:3])
install.packages("survival")
#Plantilla para el preprocesado de datos
#Importar el dataset
#Creamos también una variable para "guardar" el conjunto de datos.
dataset = read.csv("Data.csv")
#Tratamiento de los valores NA
# Con el signo del dólar podemos acceder a los objetos dentro de la variable dataset (USAR EL TABULADOR).
# Luego nos preguntamos si hay algún NA en la fila de Age y vamos a usar la función ifelse. Esta función ejecuta la primera parte
# del código si la condición es verdadera o bien, la segunda si la condición es falsa.
# La pregunta que haremos a la función ifelse es saber si hay NA en dataset$Age --> is.na (Hay NAs?).
# Si es verdadera se ejecutará lo que haya entre la primera y la segunda coma,
# y si es falso, se ejecuta lo que hay luego de la segunda coma hasta el paréntesis final. Quedaría:
# dataset$Age = ifelse(is.na(dataset$Age), Primera_condicion , Segunda_condicion )
# El primer parámetro será la función ave, que nos permitirá obtener un promedio del factor (columna)
# mediante la función FUN que a cada x, lo reemplaza con la media de la columna, sin incluir los NAs,
# es decir --> na.rm = TRUE. Quedaría:
#dataset$Age = ifelse(is.na(dataset$Age),
#               ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
#               , Segunda_condicion)
# Si no se encuentra ningún NA, entonces se no se tocaría el código y se mantendría el dataset$Age. Quedaría:
#dataset$Age = ifelse(is.na(dataset$Age),
#               ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
#               , dataset$Age)
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
# Hacemos lo mismo para dataset$salary
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
# Codificar las variables categóricas. En este caso vamos a usar la función factor.
# Esta función representa categorías directamente, por lo que necesita que se indique los levels
# y luego las etiquetas numéricas que se asignen a cada level.
# factor (variable , levels = c(), labels = c())
# En este caso, el programa sabe que la función "factor" representa categorías
# por lo que no es necesario indicar que las etiquetas numéricas no son realmente "números" (que se puedan sumar, etc.)
# Notar que tanto levels como labels son vectores (c)
dataset$Country = factor(dataset$Country,
levels = c("France" , "Spain" , "Germany"),
labels = c(1 , 2 , 3))
# Ahora también hacemos lo mismo con el factor "Purchase" y transformamos a factor 0 el "no" y 1 el "yes"
dataset$Purchased = factor(dataset$Purchased,
levels = c("No" , "Yes"),
labels = c(0 , 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test.
# Vamos a usar caTools
#install.packages("caTools") #Lo he tenido que instalar porque no lo tenía.
# Para cargar la librería usamos library(caTools).
# Como no tenemos la opción de indicar una semilla en la función sample.split, vamos a generarla antes con set.seet(entero)
# En este caso, siempre que usemos la misma semilla, se generará la misma división de datos.
# También vemos que esta función solo acepta la variable a predecir (y = purchased)
# y que el SpliRatio indica la cantidad de datos para entrenamiento (en Python indicamos la cantidad para test).
# Al final, guardamos el return en una variable llamada split
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# Como todo sigue guardado en dataset y no se generan nuevas variables, ahora vamos a generar un subconjunto del dataset original
# donde recogeremos los valores en los que la variable split sea TRUE o FALSE y los guardaremos en una nueva variable.
# Así training_set será un subset de dataset y comparará los datos generados por sample.split
# en aquellos que split sea TRUE, serán guardados en training_set y lo mismo con testing_set
training_set = subset (dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores. Nuevamente,la idea es que las variables no dominen sobre las otras.
# Podemos estandarizar o normalizar. Vamos a usar una función del paquete base.
# Pero en este caso tenemos que recordar que solo podemos escalar las columnas numéricas y la columna 1 y 4 son niveles de factor.
# Por eso, ahora vamos a escalar solamente las columnas 2 y 3. Todas las filas se indica con la coma directamente
# (a diferencia que Python que requiere :) y luego indicamos las dos columnas a escalar
training_set[, 2:3] = scale(training_set[, 2:3])
testing_set[, 2:3] = scale(testing_set[, 2:3])
View(dataset)
View(testing_set)
View(testing_set)
View(training_set)
install.packages("caTools")
install.packages("tidyverse")
library(caTools)
library(dplyr)
library(tidyverse)
#Plantilla para el preprocesado de datos
#Importar el dataset
#Creamos también una variable para "guardar" el conjunto de datos.
dataset = read.csv("Data.csv")
#Tratamiento de los valores NA
# Con el signo del dólar podemos acceder a los objetos dentro de la variable dataset (USAR EL TABULADOR).
# Luego nos preguntamos si hay algún NA en la fila de Age y vamos a usar la función ifelse. Esta función ejecuta la primera parte
# del código si la condición es verdadera o bien, la segunda si la condición es falsa.
# La pregunta que haremos a la función ifelse es saber si hay NA en dataset$Age --> is.na (Hay NAs?).
# Si es verdadera se ejecutará lo que haya entre la primera y la segunda coma,
# y si es falso, se ejecuta lo que hay luego de la segunda coma hasta el paréntesis final. Quedaría:
# dataset$Age = ifelse(is.na(dataset$Age), Primera_condicion , Segunda_condicion )
# El primer parámetro será la función ave, que nos permitirá obtener un promedio del factor (columna)
# mediante la función FUN que a cada x, lo reemplaza con la media de la columna, sin incluir los NAs,
# es decir --> na.rm = TRUE. Quedaría:
#dataset$Age = ifelse(is.na(dataset$Age),
#               ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
#               , Segunda_condicion)
# Si no se encuentra ningún NA, entonces se no se tocaría el código y se mantendría el dataset$Age. Quedaría:
#dataset$Age = ifelse(is.na(dataset$Age),
#               ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
#               , dataset$Age)
dataset$Age = ifelse(is.na(dataset$Age),
ave(dataset$Age, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Age)
# Hacemos lo mismo para dataset$salary
dataset$Salary = ifelse(is.na(dataset$Salary),
ave(dataset$Salary, FUN = function(x) mean(x, na.rm = TRUE)),
dataset$Salary)
# Codificar las variables categóricas. En este caso vamos a usar la función factor.
# Esta función representa categorías directamente, por lo que necesita que se indique los levels
# y luego las etiquetas numéricas que se asignen a cada level.
# factor (variable , levels = c(), labels = c())
# En este caso, el programa sabe que la función "factor" representa categorías
# por lo que no es necesario indicar que las etiquetas numéricas no son realmente "números" (que se puedan sumar, etc.)
# Notar que tanto levels como labels son vectores (c)
dataset$Country = factor(dataset$Country,
levels = c("France" , "Spain" , "Germany"),
labels = c(1 , 2 , 3))
# Ahora también hacemos lo mismo con el factor "Purchase" y transformamos a factor 0 el "no" y 1 el "yes"
dataset$Purchased = factor(dataset$Purchased,
levels = c("No" , "Yes"),
labels = c(0 , 1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test.
# Vamos a usar caTools
#install.packages("caTools") #Lo he tenido que instalar porque no lo tenía.
# Para cargar la librería usamos library(caTools).
# Como no tenemos la opción de indicar una semilla en la función sample.split, vamos a generarla antes con set.seet(entero)
# En este caso, siempre que usemos la misma semilla, se generará la misma división de datos.
# También vemos que esta función solo acepta la variable a predecir (y = purchased)
# y que el SpliRatio indica la cantidad de datos para entrenamiento (en Python indicamos la cantidad para test).
# Al final, guardamos el return en una variable llamada split
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.8)
# Como todo sigue guardado en dataset y no se generan nuevas variables, ahora vamos a generar un subconjunto del dataset original
# donde recogeremos los valores en los que la variable split sea TRUE o FALSE y los guardaremos en una nueva variable.
# Así training_set será un subset de dataset y comparará los datos generados por sample.split
# en aquellos que split sea TRUE, serán guardados en training_set y lo mismo con testing_set
training_set = subset (dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores. Nuevamente,la idea es que las variables no dominen sobre las otras.
# Podemos estandarizar o normalizar. Vamos a usar una función del paquete base.
# Pero en este caso tenemos que recordar que solo podemos escalar las columnas numéricas y la columna 1 y 4 son niveles de factor.
# Por eso, ahora vamos a escalar solamente las columnas 2 y 3. Todas las filas se indica con la coma directamente
# (a diferencia que Python que requiere :) y luego indicamos las dos columnas a escalar
training_set[, 2:3] = scale(training_set[, 2:3])
testing_set[, 2:3] = scale(testing_set[, 2:3])
setwd("~/Documents/BIG DATA/@Repositorios/machinelearning-az/datasets/Part 3 - Classification/Section 14 - Logistic Regression")
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Regresión Logística
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Dividir los datos en conjunto de entrenamiento y conjunto de test
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
training_set[,1:2] = scale(training_set[,1:2])
testing_set[,1:2] = scale(testing_set[,1:2])
# Ajustar el modelo de regresión logística con el conjunto de entrenamiento.
classifier = glm(formula = Purchased ~ .,
data = training_set,
family = binomial)
# Predicción de los resultados con el conjunto de testing
prob_pred = predict(classifier, type = "response",
newdata = testing_set[,-3])
y_pred = ifelse(prob_pred> 0.5, 1, 0)
# Crear la matriz de confusión
cm = table(testing_set[, 3], y_pred)
# Visualización del conjunto de entrenamiento
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Clasificación (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualización del conjunto de testing
set = testing_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Clasificación (Conjunto de Testing)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
setwd("~/Documents/BIG DATA/@Repositorios/machinelearning-az/datasets/Part 3 - Classification/Section 14 - Logistic Regression")
# Regresión Logística
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Dividir los datos en conjunto de entrenamiento y conjunto de test
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
training_set[,1:2] = scale(training_set[,1:2])
testing_set[,1:2] = scale(testing_set[,1:2])
# Ajustar el modelo de regresión logística con el conjunto de entrenamiento.
classifier = glm(formula = Purchased ~ .,
data = training_set,
family = binomial)
# Predicción de los resultados con el conjunto de testing
prob_pred = predict(classifier, type = "response",
newdata = testing_set[,-3])
y_pred = ifelse(prob_pred> 0.5, 1, 0)
# Crear la matriz de confusión
cm = table(testing_set[, 3], y_pred)
# Visualización del conjunto de entrenamiento
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Clasificación (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualización del conjunto de testing
set = testing_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Clasificación (Conjunto de Testing)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Random Forest
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Codificar la variable de clasificación como factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0,1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Escalado de valores
training_set[,1:2] = scale(training_set[,1:2])
testing_set[,1:2] = scale(testing_set[,1:2])
# Ajustar el Random Forest con el conjunto de entrenamiento.
#install.packages("randomForest")
library(randomForest)
classifier = randomForest(x = training_set[,-3],
y = training_set$Purchased,
ntree = 10)
# Predicción de los resultados con el conjunto de testing
y_pred = predict(classifier, newdata = testing_set[,-3])
# Crear la matriz de confusión
cm = table(testing_set[, 3], y_pred)
# Visualización del conjunto de entrenamiento
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Random Forest (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualización del conjunto de testing
set = testing_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Random Forest (Conjunto de Testing)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Clasificación con Árboles de Decisión
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Codificar la variable de clasificación como factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0,1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Ajustar el clasificador con el conjunto de entrenamiento.
#install.packages("rpart")
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicción de los resultados con el conjunto de testing
y_pred = predict(classifier, newdata = testing_set[,-3],
type = "class")
# Crear la matriz de confusión
cm = table(testing_set[, 3], y_pred)
# Visualización del conjunto de entrenamiento
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.05)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 250)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = "class")
plot(set[, -3],
main = 'Árbol de Decisión (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualización del conjunto de testing
set = testing_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.05)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 250)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = "class")
plot(set[, -3],
main = 'Árbol de Clasificación (Conjunto de Testing)',
xlab = 'Edad', ylab = 'Sueldo Estimado',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Representación del árbol de clasificación
plot(classifier)
text(classifier)
# Clasificación con Árboles de Decisión
# Importar el dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[, 3:5]
# Codificar la variable de clasificación como factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0,1))
# Dividir los datos en conjunto de entrenamiento y conjunto de test
# install.packages("caTools")
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
testing_set = subset(dataset, split == FALSE)
# Ajustar el clasificador con el conjunto de entrenamiento.
#install.packages("rpart")
library(rpart)
classifier = rpart(formula = Purchased ~ .,
data = training_set)
# Predicción de los resultados con el conjunto de testing
y_pred = predict(classifier, newdata = testing_set[,-3],
type = "class")
# Crear la matriz de confusión
cm = table(testing_set[, 3], y_pred)
# Visualización del conjunto de entrenamiento
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.05)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 250)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = "class")
plot(set[, -3],
main = 'Árbol de Decisión (Conjunto de Entrenamiento)',
xlab = 'Edad', ylab = 'Tienda',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualización del conjunto de testing
set = testing_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.05)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 250)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set, type = "class")
plot(set[, -3],
main = 'Árbol de Clasificación (Conjunto de Testing)',
xlab = 'Edad', ylab = 'Tienda',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Representación del árbol de clasificación
plot(classifier)
text(classifier)
